## Dropout rate
Now in every epoch, some neurons’ connections are temporarily removed or in a way some neurons are temporarily disabled. Thus, they do not contribute to the forward as well as back propagation because their weights are left unchanged. This is done to avoid overfitting (converging too soon) and better generalization the model we are forming. Clearly in our case the best value of drop out found is 0.2. We see as we increase dropout rate further, we loose some valuable updation of weights and hence the accuracy decreases. Also, when drop out rate is too low below 0.2, we see a dip in accuracy with respect to validation and test set suggesting that model is moving towards early convergence and overfitting.
## L1 and L2 regularization
Now we know that cost function=Loss (say, binary cross entropy) +Regularization Part. Now regularization component puts penalty on weight matrix in case of any error. This is the error the network/model tries to minimize. If value of Regularization parameter is too high, then a very high penalty is put on the network (penalize the absolute value of the weights) and value of some weight matrices equal close to zero. Such characteristic is undesirable and won’t encourage the model to learn where as, if the value of Regularization parameter is too low, it wont effect model much even if starts encountering errors. Hence an optimized value is required for the appropriate learning of the network. L2 Regularization is better than the L1 as it has lambda term(optimized) and is like a decay function of weights (weights do not immediately go to zero, instead gradually decrease towards zero. Same characteristic is observed in our model. Optimized value is at L2=0.0005. If we increase or decrease, we will be moving the model towards underfitting or overfitting respectively as seen in accuracies.
## Activation Functions 
Activations functions decide after calculating weighted sum of inputs (adding a bias to it), whether neurons should fire or not. Sigmoid is the nonlinear function whose value ranges from 0 to 1. It has smoother curve which gives analog activation and smooth gradient. It has tendency to move value between (-2,2) towards either side of curve (great for classification). As we see in our case, Sigmoid gives a maximum accuracy with (dropout=0.2 and l2=0.0005). The same parameters also give optimized network accuracies with Tanh and ReLU. Tanh activation is scaled Sigmoid function with gradient stronger than Sigmoid. According to our model, all the three activations functions give similar performances with ReLU slightly edging in front. This could be because ReLU(non-linear function) gives us the advantage of sparse and efficient activation in case of dense  activation when we don’t want all neurons to fire. It is also cost efficient and allows for stacking up of layers as seen in our results. The accuracies stand at 75-78% with embedding layer not trainable and 80-82% when trainable. Thus, Activation functions are helping the model to better classify the Dataset into positive and negative classes.

